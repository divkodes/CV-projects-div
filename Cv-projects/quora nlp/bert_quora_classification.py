# -*- coding: utf-8 -*-
"""bert-quora-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11G6hP4nnHCgzSL1LaFhfsBZCylnjaio-

## Classification of Quora Insincere Questions Using Transformers

## Introduction
The Quora Insincere Questions Classification task revolves around identifying and categorizing inappropriate or insincere questions on the Quora platform. Quora, a popular Q&A website, hosts diverse discussions, but not all questions are genuine. Some may be disrespectful, offensive, or intended to provoke. The aim here is to create a machine learning model that can automatically label and distinguish such insincere questions.

## The Dataset
The dataset provided for this task comprises questions sourced from Quora, labeled as either "insincere" or "sincere." Insincere questions are those that exhibit characteristics of being disrespectful, offensive, or lacking genuine intent. On the other hand, sincere questions are those that are posed with the intention of seeking information or understanding.

This dataset includes text data representing the questions, alongside binary labels indicating whether each question is insincere or sincere. It follows the familiar pattern of supervised machine learning.

## Problem Statement
The Quora Insincere Questions Classification problem is essentially a text classification task. Given a question, the goal is to predict whether the question is insincere or sincere. This task falls under the umbrella of natural language processing (NLP) and sentiment analysis. It involves interpreting the sentiment, tone, or intent behind text.

## Steps to Solve the Problem

### 1. Data Collection and Preprocessing
   - Clean and tokenize the text by removing special characters, converting to lowercase, and splitting into tokens.

### 2. Choose a Transformer Model
   - Select a pre-trained transformer model suitable for text classification (e.g., BERT, RoBERTa).


### 3. Data Formatting
   - Convert preprocessed text data into a format compatible with the chosen transformer model.
   - Tokenize text and transform it into numerical inputs that the model understands.

### 4. Fine-Tuning the Model
   - Load the pre-trained transformer model and add a classification layer on top.
   - Customize the classification layer to have the desired number of output classes (2 for sincere/insincere).
   - Define the loss function (e.g., Cross-Entropy) and optimizer (e.g., Adam) for training.

### 5. Data Splitting and Training
   - Split the dataset into training and validation sets.
   - Train the model on the training data while fine-tuning the pre-trained transformer's weights.
   - Monitor validation performance to avoid overfitting.

### 6. Hyperparameter Tuning
   - Experiment with hyperparameters like learning rate, batch size, and training epochs.
   - Implement techniques such as learning rate schedules and gradient clipping for better training stability.

### 7. Model Evaluation
   - Assess the model's performance on the validation set using appropriate metrics (accuracy, precision, recall, F1-score, etc.).
   - Adjust the model and hyperparameters based on validation results.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore")
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
import seaborn as sns
# %matplotlib inline

train = pd.read_csv('quora-insincere-questions-classification/train.csv')
test = pd.read_csv('quora-insincere-questions-classification/test.csv')

train.head()

test.head()

#Sincere Questions
train[train.target==0][:5].question_text.to_list()

#Insincere Questions
train[train.target==1][:5].question_text.to_list()

plt.figure(figsize=(22,5), dpi=100)
plt.rcParams['font.size'] = 12

palette = ['#38A7D0', '#F67088']
plt.figure(figsize=(22,5), dpi=100)
ax = sns.countplot(data =train,x=train['target'], hue=train['target'], dodge=False, palette=palette)

for bar in ax.patches:
    ax.annotate(format(bar.get_height(), '.0f'),(bar.get_x() + bar.get_width() / 2,bar.get_height()),
                 ha='center', va='center',size=15, xytext=(0, 8),textcoords='offset points')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.title('Target Class Count Plot', fontsize=20)
plt.show()

"""<center>
<div class="alert alert-block alert-danger">
As we can see there is a class imbalance in our dataset.
</div>
    </center>


## What's CLASS IMBALANCE?
Class imbalance is a common challenge encountered in many machine learning and data analysis scenarios, where the distribution of classes within a dataset is significantly skewed. In such cases, one class is often overrepresented, while the other is underrepresented, which can lead to suboptimal model performance and biased predictions.


1. **Resampling Techniques**:
   - **Oversampling**: This involves creating additional instances of the minority class by duplicating existing data points or generating synthetic examples using techniques like SMOTE (Synthetic Minority Over-sampling Technique).
   - **Undersampling**: This entails reducing the number of instances in the majority class, which may involve randomly removing data points.


"""

X = np.array(train.question_text).reshape(-1, 1)
y = np.array(train.target)

"""## **RANDOM UNDDERSAMPLING**



Here's how random undersampling works:

 In random undersampling, you randomly select a subset of samples from the majority class to match the number of samples in the minority class. The goal is to create a more balanced distribution of classes.
 The selected subset of samples from the majority class is removed, effectively reducing the number of instances in that class.

Once resampled we are creating a new dataframe and putting the resampled data into it since DataFrames are convenient to work with.
"""

rus = RandomUnderSampler(random_state=0)
X_resampled, y_resampled = rus.fit_resample(X,y)
data  = {
    'question_text':np.squeeze(X_resampled),
    'target': y_resampled
}
train_resampled = pd.DataFrame(data)
train_resampled.head()

palette = ['#38A7D0', '#F67088']
plt.figure(figsize=(22,5), dpi=100)
ax = sns.countplot(data =train_resampled,x=train_resampled['target'], hue=train_resampled['target'],
                dodge=False, palette=palette)

for bar in ax.patches:
    ax.annotate(format(bar.get_height(), '.0f'),(bar.get_x() + bar.get_width() / 2,bar.get_height()),
                 ha='center', va='center',size=15, xytext=(0, 8),textcoords='offset points')
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.title('Resampled Target Class Count Plot', fontsize=20)
plt.show()

""" the preprocessing layer and the encoder layer of BERT is loaded

**1. `bert_preprocess`:** This will load a BERT preprocessing layer using TensorFlow Hub. The preprocessing layer is responsible for preparing the input text data in a format that the BERT model can understand and process effectively. It takes care of tasks like tokenization (breaking text into individual words or subwords), padding, and creating input masks.

**2. `bert_encoder`:** This will load the core BERT encoder layer using TensorFlow Hub. The encoder is the heart of the BERT model, responsible for understanding the context and meaning of words in the input text. It transforms the tokenized and preprocessed input text into contextualized embeddings (numerical representations) that capture the relationships between words. These embeddings are crucial for downstream tasks like text classification, sentiment analysis, and more. The specific version of BERT being loaded has 12 layers, a vocabulary size of 30,000, and embeddings of size 768.

The workflow involves passing input text through the `bert_preprocess` layer to get it into the right format, then forwarding the preprocessed text through the `bert_encoder` to obtain contextualized word embeddings. These embeddings can then be used for various NLP tasks or fine-tuned for specific applications.
"""

bert_preprocess = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')
bert_encoder = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4")

"""## Embeddings


Embeddings, in the context of natural language processing (NLP), refer to numerical representations of words or other textual elements in a continuous vector space. These representations capture the semantic and contextual relationships between words, making it easier for machine learning models to process and understand text data.

**Word Embeddings:**

Word embeddings are a fundamental concept in NLP. They represent words as dense vectors in a high-dimensional space, where similar words are located close to each other. Each dimension in the vector space corresponds to a specific linguistic feature or context. For example, in a well-trained word embedding, words with similar meanings or roles might have similar vector coordinates.

**BERT (Bidirectional Encoder Representations from Transformers) and Text Classification:**

Here's how BERT's embeddings play a role in text classification:

1. **Contextual Understanding:** BERT generates embeddings that capture the context and meaning of words in a sentence. This means that each word's embedding takes into account the entire sentence it appears in. This is extremely beneficial for capturing the complex and nuanced meaning of text.

2. **Input Representation:** When using BERT for text classification, you provide the entire sentence as input. BERT processes the sentence and generates embeddings for each word. These contextualized word embeddings form the foundation of the input representation for the downstream tasks like classification.

3. **Attention Mechanism:** BERT's architecture includes an attention mechanism that helps it weigh the importance of different words in relation to each other. This mechanism ensures that the embeddings capture not only individual word meanings but also the interactions between words.

4. **Transfer Learning:** BERT is pre-trained on a large corpus of text data before fine-tuning it for specific tasks like text classification. This pre-training enables the model to learn a broad understanding of language and context, making it highly effective for a wide range of NLP tasks.

5. **Improved Performance:** Because BERT's embeddings capture context, nuances, and relationships, it performs exceptionally well on text classification tasks. It can understand subtle language patterns, idiomatic expressions, and the overall tone of the text, leading to improved classification accuracy.


"""

def get_sentence_embeding(sentences):
    preprocessed_text = bert_preprocess(sentences)
    return bert_encoder(preprocessed_text)['pooled_output']

get_sentence_embeding([
    "AI is making great strides recently especially the LLM space",
    "India is a great country with so much diversity and so many cultures"]
)

"""Let's now split the data into train and test set. The test set that we already have will be then considered as the validation set after testing the model on the then splitted data aka `X_test`"""

X_train, X_test, y_train, y_test = train_test_split(train_resampled['question_text'],train_resampled['target'])

"""## Model"""

# Bert layers
text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
preprocessed_text = bert_preprocess(text_input)
outputs = bert_encoder(preprocessed_text)

# Neural network layers
l = tf.keras.layers.Dropout(0.1, name="dropout")(outputs['pooled_output'])
l = tf.keras.layers.Dense(1, activation='sigmoid', name="output")(l)

# Use inputs and outputs to construct a final model
model = tf.keras.Model(inputs=[text_input], outputs = [l])

model.summary()

"""## Model Architecture

Let's go over the architecture for a little bit. The model's architecture is designed to process and analyze text data using a combination of two powerful techniques: BERT (Bidirectional Encoder Representations from Transformers) and a neural network.

**1. BERT Layers:**
- BERT is a language model that's really good at understanding the context and meaning of words in a sentence. It's like a super-smart language decoder that can figure out the relationships between words.
- In this architecture, we start by giving BERT a piece of text as input. Think of it as feeding a sentence or paragraph into a machine.
- The `text_input` is where you provide the text that you want the model to understand. It's like the initial door through which you let your text data in.
- The `bert_preprocess` step is where the text gets prepared for BERT to work its magic. It's like giving BERT the tools it needs to understand the language.
- The `bert_encoder` processes the preprocessed text and generates outputs that represent the text's meaning. You can think of this step as BERT's "thinking process" where it figures out what the text is saying.

**2. Neural Network Layers:**
Dropout and Dense layers have been used in front of the BERT layer to apply transfer learning in fine tuning the transformer for our task


## **Functional API**

We are using the Functional API here as opposed to the Sequential API In tensorflow. The Functional API lets us define and connect layers in a flexible and customizable way.



"""

model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

"""## Training the Model


"""

history = model.fit(X_train, y_train, epochs=3, batch_size= 100)

plt.figure(figsize=(22,5), dpi=100)
plt.plot(range(len(history.history['loss'])),history.history['loss'] , marker='o')
plt.legend(['loss'])
plt.xlabel('Epochs')
plt.xticks(range(len(history.history)+1))
plt.ylabel('Loss')
plt.title('Loss Plot', fontsize=12)
#Turning off the top and right spines
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.show()

plt.figure(figsize=(22,5), dpi=100)
plt.plot(range(len(history.history['accuracy'])),history.history['accuracy'] , marker='o', color='orange')
plt.legend(['accuracy'])
plt.xlabel('Epochs')
plt.xticks(range(len(history.history)+1))
plt.ylabel('Accuracy')
plt.title('Accuracy Plot', fontsize=12)
#Turning off the top and right spines
plt.gca().spines['top'].set_visible(False)
plt.gca().spines['right'].set_visible(False)
plt.show()

"""## Evaluating the Model
Evaluate how the model performs on the `X_test` data
"""

print("X_test Dataset Evaluation")
result = model.evaluate(X_test, y_test)
dict(zip(model.metrics_names, result))

"""Validate the model on our `test` data"""

questions = test.question_text
val_preds = model.predict(questions[:20])

print('Making Predictions on the Validation Dataset \n')
for question, prediction in zip(questions[:20], val_preds):
    prediction_label = 1 if prediction >= 0.5 else 0
    print(f"Question: {question}\nPrediction: {prediction_label}\n{'-' * 50}")